### (#52) Mobile robot navigation

Ability to move in space avoiding obstacles (static or dynamic)

Solving robot navigation involves answering 3 questions: Where am I? Where am I going? How do I get there? 

Two categories:

* Map-based navigation: the robot uses a map of environment, i.e. it has apriori knowledge of the global environment and obstacles and uses it to plan the path to the destination 
* Reactive navigation: the robot uses only local info from its surroundings obtained from sensors

### (#53) Map-based navigation

3 fundamental navigation functions to answer the 3 questions:

* Where am I? i.e. **Localization**: the  robot needs to obtain its location expressed as symbolic reference relative to the local environment e.g. "centre of the room", as topological coordinates e.g. "room 23", or in absolute coordinates. This is obtained from some sensor input e.g. GPS, laser range finders, cameras, ultrasound sensors, received signal strength, etc.  

* Where am I going? i.e. Finding the destination location. In map-based navigation the robot uses a map to recognize its environment and provide meaning to directions and locations. The associated function is **Mapping**:  the map can be loaded manually in memory e.g. with a graph or matrix representation, or generated gradually during exploration. 
* How do I get there? i.e. **Motion planning (or Path planning)**: planning the path to the destination avoiding obstacles. Requires specifying well the goal pose/target destination, either using coordinates absolute (in global frame) or relative (e.g. in the robot frame), or some other addressing scheme understood by the robot e.g "go to room 23". 

**SLAM (Simultaneous Localization And Mapping)** is the process of building a map from range sensors while the robot is exploring an unknown area. Range sensors measure distance to obstacles relative to the robot and store them in a data structure that the robot keeps updating as it moves, based on the estimations of its own location and the distance to obstacles. This process typically uses sensor fusion and filtering techniques e.g. Kalman, particle filters to merge range sensor measurements and odometry and attenuate errors to improve the estimation.

Three main ROS packages in the navigation/SLAM stack:

* `move_base`: allows the robot to navigate in a map towards a goal pose with respect to a given reference frame

* `gmapping`: creates a map using laser scan data

* `amcl`:  localization using an existing map



### (#54) SLAM Demo

1. #### Launch the gazebo simulation environment for Turtlebot3 waffle in a house scenario:

```bash
(Terminal 1) $ export TURTLEBOT3_MODEL=waffle
(Terminal 1) $ roslaunch turtlebot3_gazebo turtlebot3_house.launch
```

2. #### Launch SLAM:   

```bash
(Terminal 2) $ roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping
```

ROS supports several SLAM packages implementing different algorithms, e.g. :

* `gmapping`: a ROS wrapper for OpenSLAM's gmapping algorithm for laser-based SLAM. ([Docs](http://wiki.ros.org/gmapping))
* `cartographer`: real-time 2D & 3D SLAM by Google for multiple platforms & sensor configurations ([Docs](https://google-cartographer-ros.readthedocs.io/en/latest/))
* `hector_slam`: does not require odometry ([Docs](http://wiki.ros.org/hector_slam))

Here we use `gmapping`. This package can build a 2D occupancy grid map from laser range sensor data and pose data collected by a mobile robot. 

The map is a 2D matrix representing a grid of cells (size depends on resolution e.g. 50cm ). Each cell has 3 possible states: Unknown (-1, gray), Empty (1, white) or Occupied (0, black) 

Initially all cells are marked as Unknown meaning unexplored or yet undecided. The robot starts laser scanning. In every scan, it measures the distance to the obstacles and marks cells corresponding to the obstacle as occupied and all others in the path as free. Sometimes laser feedback is not accurate (e.g. obstacle out of range) so cells remain in unknown state. 

In practice for each cell a probability of occupancy 0-100% is calculated and thresholds are defined to determine when a cell can be marked empty or occuppied

3. #### Launch Teleop:

```bash
(Terminal 3) $ roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
```

Move the robot around using the keyboard to build the map while moving. Uses odometry to update robot pose and interpret position of obstacles as it builds the map. Tb3 has a good laser scanner: 4m range, 360deg.

4. #### (#55) Save the map generated:

```bash
(Terminal 4) $ rosrun map_server map_saver -f ./mymap
```

This generates a [`mymap.pgm`](./assets/sources/mymap.pgm) image file and a  [`mymap.yaml`](./assets/sources/mymap.yaml) metadata file in the current directory. 

Header of the example `mymap.pgm` showing file format, resolution, image size and grayscale range (max value) 

```
P5
# CREATOR: map_saver.cpp 0.050 m/pix
384 384
255
ÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍÍ...
```

![](./assets/images/mymap.png)

Example `mymap.yaml` showing path to the PGM file, its resolution, thresholds for free and occupied cells, etc to make grayscale image to binary

```yaml
image: ./mymap.pgm # path to PGM image file: stores values P=0-255
resolution: 0.050000 # in m/pixel
origin: [-10.000000, -10.000000, 0.000000] #2D pose of lower left pixel in image as [x,y,yaw] +yaw is countercockwise rotation
negate: 0 # if 1 B&W are reverted
occupied_thresh: 0.65 # pixels with P > 0.65*255 are occupied
free_thresh: 0.196 # pixels with P < 0.196*255 are free
```

### (#56) Understanding the SLAM launch file

Remember the command to launch SLAM was:

```bash
$ roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping
```

So lets open the `turtlebot3_slam.launch` launch file from package `turtlebot3_slam`. We can find it with:

```bash
$ roscd turtlebot3_slam 
$ cd launch
$ gedit turtlebot3_slam.launch
```

Set default arguments, launch the robot (as defined by environment variable `TURTLEBOT3_MODEL`) using bringup, start the SLAM stack depending on the method selected, with `gmapping` as default, launch RVIZ unless set to false.

Bringup launches the robot model and the state publisher node (broadcasts the frames and transformation based on URDF model)

```xml
<launch>
  <!-- Arguments -->
  <arg name="model" default="$(env TURTLEBOT3_MODEL)" doc="model type [burger, waffle, waffle_pi]"/>
  <arg name="slam_methods" default="gmapping" doc="slam type [gmapping, cartographer, hector, karto, frontier_exploration]"/>
  <arg name="configuration_basename" default="turtlebot3_lds_2d.lua"/>
  <arg name="open_rviz" default="true"/>

  <!-- TurtleBot3 -->
  <include file="$(find turtlebot3_bringup)/launch/turtlebot3_remote.launch">
    <arg name="model" value="$(arg model)" />
  </include>

  <!-- SLAM: Gmapping, Cartographer, Hector, Karto, Frontier_exploration, RTAB-Map -->
  <include file="$(find turtlebot3_slam)/launch/turtlebot3_$(arg slam_methods).launch">
    <arg name="model" value="$(arg model)"/>
    <arg name="configuration_basename" value="$(arg configuration_basename)"/>
  </include>

  <!-- rviz -->
  <group if="$(arg open_rviz)"> 
    <node pkg="rviz" type="rviz" name="rviz" required="true"
          args="-d $(find turtlebot3_slam)/rviz/turtlebot3_$(arg slam_methods).rviz"/>
  </group>
</launch>
```

If we inspect the `turtlebot3_gmapping.launch` file:

```xml
<launch>
  <!-- Arguments -->
  <arg name="model" default="$(env TURTLEBOT3_MODEL)" doc="model type [burger, waffle, waffle_pi]"/>
  <arg name="configuration_basename" default="turtlebot3_lds_2d.lua"/>
  <arg name="set_base_frame" default="base_footprint"/>
  <arg name="set_odom_frame" default="odom"/>
  <arg name="set_map_frame"  default="map"/>

  <!-- Gmapping -->
  <node pkg="gmapping" type="slam_gmapping" name="turtlebot3_slam_gmapping" output="screen">
    <param name="base_frame" value="$(arg set_base_frame)"/>
    <param name="odom_frame" value="$(arg set_odom_frame)"/>
    <param name="map_frame"  value="$(arg set_map_frame)"/>
    <rosparam command="load" file="$(find turtlebot3_slam)/config/gmapping_params.yaml" />
  </node>
</launch>
```

Set names of three main frames mandatory for navigation: `base_frame` (robot frame), `odom_frame` (odometry) and `map_frame` (global reference frame), robot model, then loads the parameters file, see full description in [gmapping docs](http://wiki.ros.org/gmapping). If we inspect `turtlebot3_slam/config/gmapping_params.yaml`:

```yaml
map_update_interval: 2.0
maxUrange: 3.0
sigma: 0.05
kernelSize: 1
lstep: 0.05
astep: 0.05
iterations: 5
lsigma: 0.075
ogain: 3.0
lskip: 0
minimumScore: 50
srr: 0.1
srt: 0.2
str: 0.1
stt: 0.2
linearUpdate: 1.0
angularUpdate: 0.2
temporalUpdate: 0.5
resampleThreshold: 0.5
particles: 100
xmin: -10.0 # map offsets: min are coords of the bottom left corner of the map
ymin: -10.0
xmax: 10.0 # max are coords of the top right corner of the map
ymax: 10.0
delta: 0.05 # map resolution is set here!
llsamplerange: 0.01
llsamplestep: 0.01
lasamplerange: 0.005
lasamplestep: 0.005
```

### (#57) Using the map to navigate

1. #### Launch the gazebo simulation environment for Turtlebot3 waffle in a house scenario:

```bash
(Terminal 1) $ export TURTLEBOT3_MODEL=waffle
(Terminal 1) $ roslaunch turtlebot3_gazebo turtlebot3_house.launch
```

2. #### Launch the Turtlebot3 navigation stack with the map created:   

```bash
(Terminal 2) $ roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=/home/mhered/mymap2.yaml
```

3. #### Position the robot in the map 
    Initially the map is loaded with the robot placed at [0,0] so we need to explicitly tell the robot its initial location. Check the actual location and orientation of the robot in the gazebo simulation, then in **RVIZ** click **2D Pose Estimation**, then click on the map to set location and orientation. Confirm that the obstacles detected by the laserscan (green dots) match with the map.

4. #### Send the robot to a Goal Location 
    In **RVIZ** with **2D Nav Goal** 

Two motion planners in the navigation stack: 

* **Global Path planner** ([global_planner](http://wiki.ros.org/global_planner?distro=noetic)) plans a path to the goal pose that is free of static obstacles.

* **Local path planner** ([base_local_planner](http://wiki.ros.org/base_local_planner)): executes the motion following the global path and avoiding dynamic obstacles. Represented by a smallish colored square where blue is free space, red are obstacles to avoid.

### (#58) The Recovery behaviour 

While local planner is following the planned global path:

- the robot may find unexpected obstacles. This triggers the **Marking process**

- the robot may find some obstacles are no longer present. e.g. during the initial moments where the robot is not yet in the correct location the laser scanner will incorrectly detect obstacles that are not real. This may affect the global path planner later. This triggers the **Clearing process**: fake obstacles are cleared during navigation

- Due to dynamic obstacles or because obstacles are inflated (cyan outlines) the robot may get stuck. This triggers the **Recovery behaviour**: e.g. moving back and forwards

(#61) `move_base` default recovery behaviors

If the robot is stuck during navigation due to obstacles or narrow passages it will switch to a configurable sequence of recovery behaviors: 

  1. conservative reset: clear out obstacles within a given region
  2. clearing rotation: in place rotation
  3. aggressive reset: remove all obstacles
  4. clearing rotation
  5. abort

### (#59) Understanding the navigation launch file

Remember the command to launch navigation stack was:

```bash
$ roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=/home/mhered/mymap2.yaml
```

So lets open the `turtlebot3_navigation.launch` launch file from package `turtlebot3_navigation`. We can find it with:

```bash
$ roscd turtlebot3_navigation 
$ cd launch
$ gedit turtlebot3_navigation.launch
```

* Set default arguments

* launch the robot (as defined by environment variable `TURTLEBOT3_MODEL`) using bringup. Bringup launches the robot model and the state publisher node (broadcasts the frames and transformation based on URDF model)

* start the map server node with the YAML map file (default to `turtlebot3_navigation/maps/map.yaml` if not provided as parameter) 

* launch `amcl`: node that locates the robot in the map. implements Adaptive Montecarlo Localization approach. Uses particle filter to integrate odometry and laser scans measurementsand estimate robot location in the map. Represented by the cloud of green dots around the robot - the smaller the cloud the more precise the estimate. `amcl.launch` is the configuration file.

* launch `move_base` navigation stack: implements global and local planner.

* launch RVIZ - unless set to false.

```xml
<launch>
  <!-- Arguments -->
  <arg name="model" default="$(env TURTLEBOT3_MODEL)" doc="model type [burger, waffle, waffle_pi]"/>
  <arg name="map_file" default="$(find turtlebot3_navigation)/maps/map.yaml"/>
  <arg name="open_rviz" default="true"/>
  <arg name="move_forward_only" default="false"/>

  <!-- Turtlebot3 -->
  <include file="$(find turtlebot3_bringup)/launch/turtlebot3_remote.launch">
    <arg name="model" value="$(arg model)" />
  </include>

  <!-- Map server -->
  <node pkg="map_server" name="map_server" type="map_server" args="$(arg map_file)"/>

  <!-- AMCL -->
  <include file="$(find turtlebot3_navigation)/launch/amcl.launch"/>

  <!-- move_base -->
  <include file="$(find turtlebot3_navigation)/launch/move_base.launch">
    <arg name="model" value="$(arg model)" />
    <arg name="move_forward_only" value="$(arg move_forward_only)"/>
  </include>

  <!-- rviz -->
  <group if="$(arg open_rviz)"> 
    <node pkg="rviz" type="rviz" name="rviz" required="true"
          args="-d $(find turtlebot3_navigation)/rviz/turtlebot3_navigation.rviz"/>
  </group>
</launch>
```

### (#60) A ROS node in python for navigation 

Note: not tested!

```python
#!/usr/bin/env python

import rospy
import actionlib
from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal
from math import radians, degrees
from actionlib_msgs.msg import *
from geometry_msgs.msg import Point

def move_to_goal(xGoal, yGoal):
    # move_to_goal() - method to move the robot to the goal location
    
	# define an actionlib client of class SimpleActionClient to send goal requests to the move_base server
	ac = actionlib.SimpleActionClient("move_base", MoveBaseAction)

	# wait for action server to come up
	while(not ac.wait_for_server(rospy.Duration.from_sec(5.0))):
		rospy.loginfo("Waiting for the move_base action server to come up")

	# create object of type MoveBaseGoal to specify the goal pose
	goal = MoveBaseGoal()

	# set up the reference frame used to specify the goal and time stamp
	goal.target_pose.header.frame_id = "map"
	goal.target_pose.header.stamp = rospy.Time.now()

	# define goal pose
	goal.target_pose.pose.position = Point(xGoal, yGoal,0)
	goal.target_pose.pose.orientation.x = 0.0
	goal.target_pose.pose.orientation.y = 0.0
	goal.target_pose.pose.orientation.z = 0.0
	goal.target_pose.pose.orientation.w = 1.0

	rospy.loginfo("Sending goal location...")
	# send the goal to move_base
    ac.send_goal(goal)

    # wait 60secs to reach destination
	ac.wait_for_result(rospy.Duration(60))

	if(ac.get_state() == GoalStatus.SUCCEEDED):
		rospy.loginfo("The robot reached the goal")
		return True
	else:
		rospy.loginfo("The robot failed to reach the goal")
		return False

if __name__ == '__main__':
	rospy.init_node('map_navigation', anonymous=False)
	# specify coordinates of goal location
    x_goal = -2.028
	y_goal = 4.022
	
    print('Starting go to goal')
	# call custom method
    move_to_goal(x_goal, y_goal)
	rospy.spin()
```

#### How to determine the coordinates of the goal pose using **RVIZ**

1. Visually counting grid cells from centre (each grid cell is 1m)
2. Selecting a goal pose for the robot in **RVIZ** using **2D Pose Estimate** then reading from `ìnitialpose` or  `amcl_pose`topics

 With the robot running open a terminal and type:
 ```bash
 $ rostopic echo initialpose
 ```

 Then select a new pose in **RVIZ** using **2D Pose Estimate**
 Take note of the x,y coordinates and heading
 Note that `initialpose` is published only when selecting a new pose with **2D Pose Estimate**. 
 On the other hand `amcl_pose` always shows the current location of the robot on the map: 

 ```bash
 $ rostopic echo amcl_pose
 ```

#### Creating an `actionlib` client to communicate with `move_base`

 The [`actionlib`](http://wiki.ros.org/actionlib) package provides tools to create servers that execute long-running goals with the ability to cancel the request during execution or get periodic feedback about how the request is progressing. Communication is asynchronous: the client sends requests to the server and can continue running while waiting for the response. Server may also send intermediate feedback to the client before the final result (be it success or failure)

 `move_base` has the global and local path planners and is an `actionlib` - not a ROS service - so that the robot can do other tasks while navigating.

The `move_base` server sends messages of type `move_base_msgs/MoveBaseAction`

````
# move_base_msgs/MoveBaseAction.msg

MoveBaseActionGoal action_goal
MoveBaseActionResult action_result
MoveBaseActionFeedback action_feedback
````

### (#62) Requirements to use navigation stack in a robot 

See http://wiki.ros.org/navigation/Tutorials/RobotSetup/):

1. ROS installed in your robot

2. robot must publish the tf transforms between sensors and base coordinate frame. Two options: through an URDF model of the robot. Or you can write a node that broadcasts the trasnform between the frames

3. sensors publish data  as `sensor_msgs/LaserScan` or `sensor_msgs/PointCloud` messages
4. robot publishes odometry as `nav_msgs/Odometry`message with Pose and Twist
5. robot can receive `geometry_msgs/Twist` messages to control it. Linear and angular velocities assumed in base coordinate frame. A node subscribes to a topic `cmd_vel ` and converts commands to motor commands
6. nav stack can work with or without a map

### Reactive navigation
