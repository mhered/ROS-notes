### (#52) Mobile robot navigation

Ability to move in space avoiding obstacles (static or dynamic)

Solving robot navigation involves answering 3 questions: Where am I? Where am I going? How do I get there? 

Two types:

* Map-based navigation: the robot uses a map of environment, i.e. it has apriori knowledge of the global environment and obstacles and uses it to plan the path to the destination 
* Reactive navigation: the robot uses only local info from its surroundings obtained from sensors

### (#53) Map-based navigation

3 fundamental navigation functions to answer the 3 questions:

* Where am I?: **Localization**: the  robot needs to obtain its location expressed as symbolic reference relative to the local environment e.g. "centre of the room", as topological coordinates e.g. "room 23", in absolute coordinates. This is done using some sensor input e.g. GPS, laser range finders, cameras, ultrasound sensors, received signal strength, etc.  

* Where am I going?: **Mapping**: robot needs a map to recognize its environment and provide meaning to directions and locations. It can be loaded manually in memory or generated gradually during exploration. There are different representations e.g. graph or matrix
* How do I get there?: **Motion planning (or Path planning)**: planning the path to the destination avoiding obstacles. Requires an address that the robot understands to specify well the goal pose/target destination. Needs an addressing scheme to specify target destination from initial location. E.g. go to a room number. Addresses can also expressed in coordinates either absolute (in global frame) or relative (e.g. in the robot frame)

**SLAM (Simultaneous Localization And Mapping)** is the process of building a map from range sensors while the robot is exploring an unknown area. Sensors measure distance to obstacles relative to the robot and store them in a data structure that the robot keeps updating it as it goes, based on the estimations of its own location and the distance to obstacles. This process typically uses sensor fusion and filtering techniques e.g. Kalman, particle filters to merge range sensor measurements and odometry and attenuate errors to improve the estimation.

Three main ROS packages  in the navigation/SLAM stack:

* `move_base`: allows the robot to navigate in a map towards a goal pose with respect to a given frame

* `gmapping`: creates maps using laser scan data

* `amcl`:  localization using an existing map



### (#54) SLAM Demo

1. #### Launch the gazebo simulation environment for Turtlebot3 waffle in a house scenario:

```bash
$ export TURTLEBOT3_MODEL=waffle
$ roslaunch turtlebot3_gazebo turtlebot3_house.launch
```

2. #### Launch SLAM:   

```bash
$ roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping
```

ROS support several SLAM packages implementing different algorithms, e.g. :

* `gmapping`: a ROS wrapper for OpenSLAM's gmapping algorithm for laser-based SLAM. ([Docs](http://wiki.ros.org/gmapping))
* `cartographer`: real-time 2D & 3D SLAM by Google for multiple platforms & sensor configurations ([Docs](https://google-cartographer-ros.readthedocs.io/en/latest/))
* `hector_slam`: does not require odometry ([Docs](http://wiki.ros.org/hector_slam))

Here we use `gmapping`. This package can build a 2D occupancy grid map from laser range sensor data and pose data collected by a mobile robot. Each cell can be free (1, white) or occupied (0, black), Unknown (-1, gray if undecided or unexplored). Cells store probability of occupancy 0-100%), size of cell depends on resolution. 

Initially all cells are marked as unknown. The robot starts laser scanning and measures the distance to the obstacle and marks cells corresponding to the obstacle as occupied and all others in the path as free. Sometimes laser feedback is not accurate (e.g. obstacle out of range) so cells remain in unknown state

3. #### Launch Teleop:

```bash
$ roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
```

Move the robot  around using the keyboard to build the map while moving. Uses odometry to update robot pose and interpret position of obstacles as it builds the map. Good laser scanner for Tb3: 4m, 360deg.

4. #### Save the map generated:

```bash
$ rosrun map_server map_saver -f ./map
```

This generates a `.pgm` image file and a  `.yaml` metadata file.

The map is a 2D matrix representing a grid of cells 5-50cm in size depending on resolution. A cell has 3 possible states: that can be empty (1, white) occupied (0, black) or unknown if undecided or not unexplored (grey). For each cell there is a probability of occupancy 0-100% or -1 if unexplored.

### (#60) A ROS node in python for navigation 

Note: not tested

```python
#!/usr/bin/env python

import rospy
import actionlib
from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal
from math import radians, degrees
from actionlib_msgs.msg import *
from geometry_msgs.msg import Point

def move_to_goal(xGoal, yGoal):
    # move_to_goal() - method to move the robot to the goal location
    
	# define an actionlib client to send goal requests to the move_base server
	ac= actionlib.SimpleActionClient("move_base", MoveBaseAction)

	# wait for action server to come up
	while(not ac.wait_for_server(rospy.Duration.from_sec(5.9))):
		rospy.loginfo("Waiting for the move_base action server to come up")

	# create object of type MoveBaseGoal
	goal = MoveBaseGoal()

	# set up the reference frame and time stamp
	goal.target_pose.header.frame_id = "map"
	goal.target_pose.header.stamp = rospy.Time.now()

	# define goal
	goal.target_pose.pose.position = Point(xGoal, yGoal,0)
	goal.target_pose.pose.orientation.x = 0.0
	goal.target_pose.pose.orientation.y = 0.0
	goal.target_pose.pose.orientation.z = 0.0
	goal.target_pose.pose.orientation.w = 1.0

	rospy.loginfo("Sending goal location...")
	ac.send_goal(goal)

    # wait 60secs to reach destination
	ac.wait_for_result(rospy.Duration(60))

	if(ac.get_state() == GoalStatus.SUCCEEDED):
		rospy.loginfo("You have reached the destination")
		return True
	else:
		rospy.loginfo("The robot failed to reach the destination")
	return False

if __name__ == '__main__':
	rospy.init_node('map_navigation', anonymous=False)
	# specify coordinates of goal location
    x_goal = -2.028
	y_goal = 4.022
	print('start go to goal')
	
    # call custom method
    move_to_goal(x_goal, y_goal)
	rospy.spin()
```

#### How to determine the coordinates of the goal pose using **RVIZ**

1. Visually counting grid cells from centre (each grid cell is 1m)
2. Reading from `Ã¬nitialpose`topic

 With the robot running open a terminal and type:
 ```bash
 $ rostopic echo initialpose
 ```

 Then select a new pose in **RVIZ** using **2D pose estimate**
 Take note of the x,y coordinates and heading
 Note that `initialpose` is published only when selecting a new pose with **2D pose estimate**. 
 On the other hand `acml_pose` always shows the current location of the robot on the map: 

 ```bash
 $ rostopic echo amcl_pose
 ```

#### Create an `actionlib` client to communicate with `move_base`

 The [`actionlib`](http://wiki.ros.org/actionlib) package provides support for client-server applications where tasks are preemptable i.e. can be interrupted. Communication is asynchronous: the client sends requests to the server and can continue running while  waiting for the response. Server may also send intermediate feedback to  the client before the final result. 
 `move_base` has the global and local path planners and is an `actionlib` not a service so that the robot can do other tasks while navigating.

#### `move_base` default recovery behaviors
 If robot is stuck during navigation due to obstacles or narrow passages it will switch to configurable recovery behaviors: 

* conservative reset: clear out obstacles within a given region
* clearing rotation: in place rotation
* aggressive reset
* clearing rotation
* abort

### Reactive navigation
