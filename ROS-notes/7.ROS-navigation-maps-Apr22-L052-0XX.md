### (#52) Mobile robot navigation

Ability to move in space avoiding obstacles (static or dynamic)

Solving robot navigation involves answering 3 questions: Where am I? Where am I going? How do I get there? 

Two types:

* Map-based navigation: the robot uses a map of environment, i.e. it has apriori knowledge of the global environment and obstacles and uses it to plan the path to the destination 
* Reactive navigation: the robot uses only local info from its surroundings obtained from sensors

### (#53) Map-based navigation

3 fundamental navigation functions to answer the 3 questions:

* Where am I?: **Localization**: the  robot needs to obtain its location expressed as symbolic reference relative to the local environment e.g. "centre of the room", as topological coordinates e.g. "room 23", in absolute coordinates. This is done using some sensor input e.g. GPS, laser range finders, cameras, ultrasound sensors, received signal strength, etc.  

* Where am I going?: **Mapping**: robot needs a map to recognize its environment and provide meaning to directions and locations. It can be loaded manually in memory or generated gradually during exploration. There are different representations e.g. graph or matrix
* How do I get there?: **Motion planning (or Path planning)**: planning the path to the destination avoiding obstacles. Requires an address that the robot understands to specify well the goal pose/target destination. Needs an addressing scheme to specify target destination from initial location. E.g. go to a room number. Addresses can also expressed in coordinates either absolute (in global frame) or relative (e.g. in the robot frame)

**SLAM (Simultaneous Localization And Mapping)** is the process of building a map from range sensors while the robot is exploring an unknown area. Sensors measure distance to obstacles relative to the robot and store them in a data structure that the robot keeps updating it as it goes, based on the estimations of its own location and the distance to obstacles. This process typically uses sensor fusion and filtering techniques e.g. Kalman, particle filters to merge range sensor measurements and odometry and attenuate errors to improve the estimation.

Three main ROS packages  in the navigation/SLAM stack:

* `move_base`: allows the robot to navigate in a map towards a goal pose with respect to a given frame

* `gmapping`: creates maps using laser scan data

* `amcl`:  localization using an existing map



### (#54) SLAM Demo

1. Launch the gazebo simulation environment for Turtlebot3 waffle in a house scenario:

```bash
$ export TURTLEBOT3_MODEL=waffle
$ roslaunch turtlebot3_gazebo turtlebot3_house.launch
```

2. Launch `gmapping` package which contains a ROS wrapper for the OpenSLAM gmapping algorithm. It can build a 2D occupancy grid map from laser range sensor data and pose data collected by a mobile robot. Each cell can be free (1, white) or occupied (0, black), Unknown (-1, gray if undecided or unexplored). Cells store probability of occupancy 0-100%), size of cell depends on resolution. 

   Initially all cells marked as unknown. Robot starts laser scanning and measures the distance to the obstacle and marks cells corresponding to the obstacle as occupied and all others in the path as free. Sometimes laser feedback is not accurate (e.g. obstacle out of range) so cells remain in unknown state

```bash
$ roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=gmapping
```

3. Launch Teleop. Move the robot  around using the keyboard to build the map while moving. Uses odometry to update robot pose and interpret position of obstacles as it builds the map. Good laser scanner for Tb3: 4m, 360deg.

```bash
$ roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
```

4. Save the map generated:

```bash
$ rosrun map_server map_saver -f ./map
```

The map is a 2D matrix representing a grid of cells 5-50cm in size depending on resolution. A cell has 3 possible states: that can be empty (1, white) occupied (0, black) or unknown if undecided or not unexplored (grey). For each cell there is a probability of occupancy 0-100% or -1 if unexplored.

This generates a `.pgm` image file and a  `.yaml` metadata file.

ROS support different SLAM algorithms:

* `gmapping`: a ROS wrapper for OpenSLAM's gmapping algorithm for laser-based SLAM. ([Docs](http://wiki.ros.org/gmapping))
* `cartographer`: real-time 2D & 3D SLAM by Google for multiple platforms & sensor configurations ([Docs](https://google-cartographer-ros.readthedocs.io/en/latest/))
* `hector_slam`: does not require odometry ([Docs](http://wiki.ros.org/hector_slam))



### (#60) A ROS node in python for navigation 

```python
import rospy
import actionlib
from move_base_msgs.msg import MoveBaseAction, MoveBaseGoal
from math import radians, degrees
from actionlib_msgs.msg import *
from geometry_msgs.msg import Point

def move_to_goal(XGoal, yGoal):
	# define a client to send goal requests to the move_base server
	ac= actionlib.SimpleActionClient("move_base", MoveBaseAction)

	# wait for action server to come up
	while(not ac.wait_for_server(rospy.Duration.from_sec(5.9))):
		rospy.loginfo("Waiting for the move_base action server to come up")

	# create object of type MoveBaseGoal
	goal = MoveBaseGoal()

	# set up the frame parameters
	goal.target_pose.header.frame_id = "map"
	goal.target_pose.header.stamp = rospy.Time.now()

	# moving towards the goal
	goal.target_pose.pose.position = Point(xGoal, yGoal,0)
	goal.target_pose.pose.orientation.x = 0.0
	goal.target_pose.pose.orientation.y = 0.0
	goal.target_pose.pose.orientation.z = 0.0
	goal.target_pose.pose.orientation.w = 1.0

	rospy.loginfo("Sending goal location...")
	ac.send_goal(goal)

	ac.wait_for_result(rospy.Duration(60))

	if(ac.get_state() == GoalStatus.SUCCEEDED):
		rospy.loginfo("You have reached the destination")
		return True
	else:
		rospy.loginfo("The robot failed to reach the destination")
	return False

if __name__ == '__main__':
	rospy.init_node('map_navigation', anonymous=False)
	x_goal = -2.028
	y_goal = 4.022
	print('start go to goal')
	move_to_goal(x_goal, y_goal)
	rospy.spin()
```

1. get goal pose using **RVIZ**

With the robot running:
```
$ rostopic echo initialpose
```

Then select a new pose in **RVIZ** using **2D pose estimate**

### Reactive navigation
